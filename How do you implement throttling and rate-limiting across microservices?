How do you implement throttling and rate-limiting across microservices?

Below is a clear, practical guide to implementing throttling and rate-limiting across microservices in Java, including architecture patterns and sample code using popular libraries (Redis, Bucket4j, Spring Boot filters, API Gateway patterns).

✅ 1. What Problem Are We Solving?
In microservices, you usually need to control:

Client-side rate limiting
Limit how many requests a client/system can send (per API key, per user, per IP, etc.)

Server-side throttling
Ensure a service does not exceed its own capacity (to avoid overload).

Distributed rate-limiting
Rate limits need to be applied across all service instances → requires shared storage such as Redis.

✅ 2. High-Level Architecture Pattern
Use a shared store to keep counters/tokens:

Client → API Gateway → Microservice A → Microservice B
                |           |                |
              Token bucket / Sliding window (central store: Redis)
Common ways:

API Gateway (Istio/Nginx/Kong/Apigee) – best for all services

Sidecar / Service mesh – native Envoy throttling

Application-level rate limiting using Java libraries

Bucket4j

Resilience4j RateLimiter

Guava RateLimiter (simple but not distributed)

✅ 3. Distributed Rate Limiting Using Redis + Bucket4j (recommended)
Why Bucket4j + Redis?
Token bucket algorithm (most accurate for API throttling)

Cluster-safe

Very fast (uses Redis atomic operations)

⭐ 3.1. Add Dependencies
<!-- Bucket4j with Redis -->
<dependency>
    <groupId>com.bucket4j</groupId>
    <artifactId>bucket4j-redis</artifactId>
    <version>8.4.0</version>
</dependency>

<dependency>
    <groupId>org.redisson</groupId>
    <artifactId>redisson</artifactId>
    <version>3.25.2</version>
</dependency>
⭐ 3.2. Create Redis Client
@Configuration
public class RedisConfig {

    @Bean
    public RedissonClient redissonClient() {
        Config config = new Config();
        config.useSingleServer()
              .setAddress("redis://localhost:6379");
        return Redisson.create(config);
    }
}
⭐ 3.3. Create a Distributed Rate Limiter Service
@Service
public class RateLimiterService {

    private final RedissonClient redissonClient;

    public RateLimiterService(RedissonClient client) {
        this.redissonClient = client;
    }

    public Bucket resolveBucket(String key) {

        RMap<String, byte[]> map = redissonClient.getMap("rate-limits");

        BucketConfiguration config = BucketConfiguration.builder()
                .addLimit(Bandwidth.classic(
                        10,                      // 10 requests
                        Refill.greedy(10, Duration.ofSeconds(1)) // per 1 sec
                ))
                .build();

        return Bucket4j.extension(RedisBucket4jExtension.getInstance())
                .builder()
                .addLimit(config.getBandwidths().get(0))
                .build(map, key.getBytes());
    }
}
This gives a per-client limit of 10 req/sec across all service instances.

⭐ 3.4. Add a Spring Filter to Enforce It
@Component
public class RateLimitFilter extends OncePerRequestFilter {

    @Autowired
    private RateLimiterService rateLimiterService;

    @Override
    protected void doFilterInternal(HttpServletRequest request,
                                    HttpServletResponse response,
                                    FilterChain filterChain)
                                    throws ServletException, IOException {

        String apiKey = request.getHeader("X-API-Key");
        if (apiKey == null) apiKey = "anonymous";

        Bucket bucket = rateLimiterService.resolveBucket(apiKey);

        if (bucket.tryConsume(1)) {
            filterChain.doFilter(request, response);
        } else {
            response.setStatus(HttpStatus.TOO_MANY_REQUESTS.value());
            response.getWriter().write("Rate limit exceeded");
        }
    }
}
Now every request is checked in Redis.

✅ 4. Using Resilience4j for Method-level Throttling
Great for service-to-service throttling inside microservices.

Add Dependency
<dependency>
    <groupId>io.github.resilience4j</groupId>
    <artifactId>resilience4j-ratelimiter</artifactId>
    <version>2.2.0</version>
</dependency>
Configuration
resilience4j.ratelimiter:
  backendA:
    limitForPeriod: 50
    limitRefreshPeriod: 1s
    timeoutDuration: 0
Use Annotation
@RateLimiter(name = "backendA")
public String callAnotherService() {
    return restTemplate.getForObject("http://service-b/api", String.class);
}
This prevents a microservice from hammering another service.

✅ 5. Global Throttling Using Spring Cloud Gateway
If you use a gateway, apply rate limiting in front of microservices.

Add dependency
<dependency>
  <groupId>org.springframework.cloud</groupId>
  <artifactId>spring-cloud-starter-gateway</artifactId>
</dependency>
Config with Redis-based Rate Limiter
spring:
  cloud:
    gateway:
      routes:
        - id: serviceA
          uri: http://localhost:8081
          predicates:
            - Path=/serviceA/**
          filters:
            - name: RequestRateLimiter
              args:
                redis-rate-limiter.replenishRate: 10
                redis-rate-limiter.burstCapacity: 20
                key-resolver: "#{@ipKeyResolver}"
Key resolver:

@Bean
KeyResolver ipKeyResolver() {
    return exchange -> Mono.just(exchange.getRequest().getRemoteAddress().getHostName());
}
This applies rate limiting at the gateway level.

✅ 6. Client-Side Backpressure (Optional)
Use Reactor to prevent internal overload:

Flux.range(1, 1000)
    .limitRate(50)  // process only 50 at a time
    .flatMap(i -> webClient.get()...)
    .subscribe();
✅ Summary (Best Practices)
Layer	Technique	When to Use
API Gateway	Redis rate limiter (NGINX, Spring Gateway, Kong)	Best for global uniform limits
Microservices	Bucket4j + Redis	Precise per-client or per-user rate limiting
Call between services	Resilience4j	Prevent Service A flooding Service B
Service Mesh	Envoy/Istio throttling	Platform-level control
If you want, I can also generate:

