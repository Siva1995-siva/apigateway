Your service calls 5 other services — how do you design to avoid cascading failures?
Nice — great question. When your service depends on five downstream services, a single failure can ripple out and take everything down. Below is a practical, copy-pasteable Java/Spring Boot design + code that shows the most important patterns to prevent cascading failures: timeouts, retries with jitter, circuit breakers, bulkheads (thread pool isolation), rate limiting, fallbacks, async + aggregation, caching, and load-shedding. I use Resilience4j + Feign + Caffeine cache as the primary libraries (these are modern, lightweight, and widely used).

High level design (summary)

Timebox every call (timeouts) — never wait forever.

Retry with exponential backoff + jitter — but only for idempotent calls.

Circuit breakers — fail fast when downstream is unhealthy.

Bulkheads (thread-pool) per downstream — avoid blocking all caller threads.

Fallbacks / graceful degradation — return cached or partial data instead of failing.

Async + aggregate — call downstreams in parallel and combine results with a deadline.

Rate limiting + load shedding — prevent overload under high load.

Observability — metrics, logs, distributed tracing, health checks.

Below is a concrete example: an AggregatorService exposes /aggregate/{id} and calls five services (svc-a..svc-e). It uses Feign for REST calls, Resilience4j for resilience patterns, Caffeine for short-term cache, and thread-pool bulkheads to isolate callers.

pom.xml (relevant deps)
<dependencies>
  <!-- Spring Boot web + openfeign -->
  <dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-web</artifactId>
  </dependency>
  <dependency>
    <groupId>org.springframework.cloud</groupId>
    <artifactId>spring-cloud-starter-openfeign</artifactId>
  </dependency>

  <!-- Resilience4j modules -->
  <dependency>
    <groupId>io.github.resilience4j</groupId>
    <artifactId>resilience4j-spring-boot3</artifactId>
    <version>2.0.5</version>
  </dependency>
  <dependency>
    <groupId>io.github.resilience4j</groupId>
    <artifactId>resilience4j-feign</artifactId>
    <version>2.0.5</version>
  </dependency>

  <!-- Caffeine cache -->
  <dependency>
    <groupId>com.github.ben-manes.caffeine</groupId>
    <artifactId>caffeine</artifactId>
  </dependency>

  <!-- Optional: Micrometer for metrics -->
  <dependency>
    <groupId>io.micrometer</groupId>
    <artifactId>micrometer-registry-prometheus</artifactId>
  </dependency>
</dependencies>

application.yml — resilience & feign timeouts + cache
feign:
  client:
    config:
      default:
        connectTimeout: 2000
        readTimeout: 2000

resilience4j:
  circuitbreaker:
    configs:
      default:
        registerHealthIndicator: true
        slidingWindowType: TIME_BASED
        slidingWindowSize: 30s
        failureRateThreshold: 50
        waitDurationInOpenState: 10s
        permittedNumberOfCallsInHalfOpenState: 3
        minimumNumberOfCalls: 5
  retry:
    configs:
      default:
        maxAttempts: 3
        waitDuration: 500ms
        exponentialBackoffMultiplier: 2.0
        retryExceptions:
          - java.io.IOException
          - java.net.SocketTimeoutException
  bulkhead:
    threadPool:
      # one thread pool per downstream service (names used in code below)
      svcA:
        coreThreadPoolSize: 5
        maxThreadPoolSize: 10
        queueCapacity: 50
      svcB:
        coreThreadPoolSize: 5
        maxThreadPoolSize: 10
        queueCapacity: 50
      svcC:
        coreThreadPoolSize: 3
        maxThreadPoolSize: 6
        queueCapacity: 30
      svcD:
        coreThreadPoolSize: 3
        maxThreadPoolSize: 6
        queueCapacity: 30
      svcE:
        coreThreadPoolSize: 2
        maxThreadPoolSize: 4
        queueCapacity: 20
  timelimiter:
    configs:
      default:
        timeoutDuration: 2s

management:
  endpoints:
    web:
      exposure:
        include: health,prometheus

Feign clients (one per downstream)
@FeignClient(name = "svc-a", url="${svc.a.url:http://localhost:8081}")
public interface SvcAClient {
    @GetMapping("/a/{id}")
    SvcAResponse getA(@PathVariable("id") String id);
}

@FeignClient(name = "svc-b", url="${svc.b.url:http://localhost:8082}")
public interface SvcBClient {
    @GetMapping("/b/{id}")
    SvcBResponse getB(@PathVariable("id") String id);
}

// same pattern for SvcCClient, SvcDClient, SvcEClient

AggregatorService — orchestration with resilience4j decorators + thread-pool bulkhead
@Service
@RequiredArgsConstructor
public class AggregatorService {

    private final SvcAClient svcA;
    private final SvcBClient svcB;
    private final SvcCClient svcC;
    private final SvcDClient svcD;
    private final SvcEClient svcE;

    // lightweight in-process cache
    private final Cache<String, AggregatedResponse> cache = Caffeine.newBuilder()
            .expireAfterWrite(Duration.ofSeconds(30))
            .maximumSize(5_000)
            .build();

    // Obtain resilience4j registries via Spring injection (auto-configured)
    private final CircuitBreakerRegistry cbRegistry;
    private final ThreadPoolBulkheadRegistry bulkheadRegistry;
    private final TimeLimiterRegistry timeLimiterRegistry;
    private final RetryRegistry retryRegistry;

    public CompletableFuture<AggregatedResponse> aggregate(String id) {
        // check short-lived cache first (fast fail)
        AggregatedResponse cached = cache.getIfPresent(id);
        if (cached != null) {
            return CompletableFuture.completedFuture(cached);
        }

        // call each downstream concurrently but isolated in thread-pool bulkheads
        CompletableFuture<SvcAResponse> aFuture = callWithResilience("svcA", () -> svcA.getA(id), SvcAResponse.empty());
        CompletableFuture<SvcBResponse> bFuture = callWithResilience("svcB", () -> svcB.getB(id), SvcBResponse.empty());
        CompletableFuture<SvcCResponse> cFuture = callWithResilience("svcC", () -> svcC.getC(id), SvcCResponse.empty());
        CompletableFuture<SvcDResponse> dFuture = callWithResilience("svcD", () -> svcD.getD(id), SvcDResponse.empty());
        CompletableFuture<SvcEResponse> eFuture = callWithResilience("svcE", () -> svcE.getE(id), SvcEResponse.empty());

        // set an overall deadline: gather all results but don't wait more than 1.8s
        CompletableFuture<AggregatedResponse> all = CompletableFuture.allOf(aFuture, bFuture, cFuture, dFuture, eFuture)
            .orTimeout(1800, TimeUnit.MILLISECONDS)
            .thenApply(v -> {
                AggregatedResponse agg = new AggregatedResponse();
                agg.setA(getSafe(aFuture));
                agg.setB(getSafe(bFuture));
                agg.setC(getSafe(cFuture));
                agg.setD(getSafe(dFuture));
                agg.setE(getSafe(eFuture));
                // optional: set flags for partial data
                agg.setPartial(isPartial(agg));
                // store in cache for a short time to prevent thundering herd
                cache.put(id, agg);
                return agg;
            })
            .exceptionally(ex -> {
                // If overall deadline or other unexpected error -> return degraded response
                AggregatedResponse degraded = new AggregatedResponse();
                degraded.setMessage("degraded");
                degraded.setPartial(true);
                return degraded;
            });

        return all;
    }

    private <T> T getSafe(CompletableFuture<T> f) {
        try {
            return f.getNow(null);
        } catch (CompletionException ce) {
            return null;
        }
    }

    private boolean isPartial(AggregatedResponse agg) {
        return agg.getA()==null || agg.getB()==null || agg.getC()==null || agg.getD()==null || agg.getE()==null;
    }

    /**
     * Generic wrapper that applies: threadPoolBulkhead -> timeLimiter -> retry -> circuitBreaker -> supplier
     * returns a CompletableFuture that completes with fallbackValue on failure.
     */
    private <T> CompletableFuture<T> callWithResilience(String name, Supplier<T> supplier, T fallbackValue) {
        // registries configured via application.yml
        CircuitBreaker cb = cbRegistry.circuitBreaker(name + "CB");
        ThreadPoolBulkhead bulkhead = bulkheadRegistry.bulkhead(name); // matches config svcA/svcB...
        TimeLimiter timeLimiter = timeLimiterRegistry.timeLimiter(name);
        Retry retry = retryRegistry.retry(name);

        // decorate supplier
        Callable<T> callable = () -> supplier.get();

        // apply bulkhead (thread pool isolation)
        Callable<T> bulkheadCallable = ThreadPoolBulkhead.decorateCallable(bulkhead, callable);

        // apply retry
        Callable<T> retryCallable = Retry.decorateCallable(retry, bulkheadCallable);

        // apply time limiter
        Callable<CompletionStage<T>> timeLimited = TimeLimiter.decorateFutureSupplier(timeLimiter,
            () -> CompletableFuture.supplyAsync(() -> {
                try {
                    return retryCallable.call();
                } catch (Exception e) {
                    throw new CompletionException(e);
                }
            })
        );

        // apply circuit breaker
        Supplier<CompletionStage<T>> decorated = CircuitBreaker.decorateCompletionStage(cb,
            () -> (CompletionStage<T>) timeLimited.call());

        // run it
        CompletableFuture<T> future = new CompletableFuture<>();
        try {
            decorated.get().whenComplete((result, err) -> {
                if (err != null) {
                    // fallback logic: could be static default, cached stale value, or partial response
                    future.complete(fallbackValue);
                } else {
                    future.complete(result);
                }
            });
        } catch (Exception ex) {
            future.complete(fallbackValue);
        }
        return future;
    }
}


Notes:
We use a thread-pool bulkhead per downstream (svcA..svcE) so that if svcA becomes slow, only that thread pool backs up — app threads and other downstream calls are still free.
Fallbacks are immediate and can be replaced with cached/stale data, defaults, or a "service unavailable" indicator.

Controller that exposes the aggregated endpoint
@RestController
@RequiredArgsConstructor
public class AggregatorController {

    private final AggregatorService aggregatorService;

    @GetMapping("/aggregate/{id}")
    public CompletableFuture<ResponseEntity<AggregatedResponse>> aggregate(@PathVariable String id) {
        return aggregatorService.aggregate(id)
            .thenApply(resp -> ResponseEntity.ok(resp));
    }
}

Example fallback strategies (choose 1 or combine)

Cached stale copy: keep a materialized view (L3) or short Caffeine cache. Return stale but available data.

Partial response: include fields for which data is present and mark as partial.

Default/faux data: return safe defaults for non-critical sections.

Queue the request and do background retry (for user-initiated non-blocking ops).

Why these patterns prevent cascading failures

Timeouts prevent threads from piling up waiting.

Circuit breakers stop repeated attempts to a failing resource and give it time to recover.

Bulkheads isolate latency/failures to one segment.

Retries with backoff and jitter avoid synchronized retries that amplify load.

Short caches reduce repeated calls (thundering herd) and allow graceful degradation.

Async aggregation with an overall deadline prevents slow downstreams from blocking client responses.

Rate limiting / load shedding reduce incoming pressure when system is near capacity so essential paths stay healthy.

Operational suggestions (must-haves)

Metrics & Alerts: expose circuit breaker state, bulkhead queue sizes, retry statistics, error rates (Micrometer + Prometheus + Grafana).

Distributed Tracing: use OpenTelemetry / Zipkin to see which downstream is causing latency.

Health endpoints and dependency-level health checks; use readiness probes for K8s.

Chaos testing (e.g., simulate latency/errors) to validate fallbacks and CB thresholds.

Automated failover and autoscaling (HPA) for surge resilience.

Quick checklist to implement in your codebase

 Set sensible HTTP timeouts at Feign/HTTP client level.

 Add Resilience4j: CircuitBreaker + Retry + ThreadPoolBulkhead + TimeLimiter.

 Implement fallbacks and short cache.

 Call downstreams in parallel and set an overall deadline.

 Expose CB metrics and monitor them.

 Add rate limiter in gateway if needed.

 Test failure scenarios (latency, errors, partial failures).
